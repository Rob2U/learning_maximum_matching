# GENERAL
iterations: 250000
max_code_length: 44  # = 2*punish_cap
reset_for_every_run: False
num_vms_per_env: 100
only_reward_of_first_vm: False
action_masking: True
vectorize_environment: False
device: cuda

# STATE
# currently only adds the state of the first VM to the observation. Therefore, it makes sense to set num_vms_per_env to 1 if add_vm_state_to_observations is set to True
add_vm_state_to_observations: False

# REWARDs
only_reward_on_ret: False
# For Rings you require:
# \sum_{n - 1}(PUSH,RESET,WRITE,(i - 1)*{POP,IF,WRITE},ADD) + RET
# = \sum_{i=1}^{n-1}(4 + 3*(i - 1)) + 1
# = \sum_{i=1}^{n-1}(3*i + 1) + 1
# = 3*\sum_{i=1}^{n-1}i + n - 1 + 1
# = 3*(n-1)*n/2 + n
# = 3*(n^2 - n)/2 + n
# = 3*n^2/2 - 3*n/2 + n
# = 3/2*n^2 - n/2
punish_cap: 22 # = 3/2*16 - 2

# ENVIRONMENT -- GRAPH Generation
graph_type: "ring" # "random" or "ring"
min_n: 4
min_m: 4 # only relevant for random. For ring, m is always n
max_n: 4
max_m: 4 # only relevant for random. For ring, m is always n


# MODEL
gamma: 0.99
no_feature_extractor: True
layer_dim_pi: 512
layer_dim_vf: 512
feature_dim: 512

fe_d_model: 512
fe_nhead: 4
fe_num_blocks: 4

beta: 0.5
reward_fn:
  # punish_code_length: 0.0
  f_score_mst: 1.0

  reward_finite_runtime: 0.0 # bad values (skew the reward)
  reward_valid_spanning_tree_length: 0.0
  reward_no_cycles: 0.0 # not implemented
  reward_covered_nodes: 0.0
  reward_minimality: 0.0 # bad values (skew the reward)
  reward_efficiency: 0.0
  reward_distance_to_MST: 0.0
  reward_connected: 0.0

  reward_correct_edges: 0.0
  punish_mst_weight_too_large: 0.0
  punish_no_improvement: 1.0

factor_fn:
  punish_code_length: 1.0